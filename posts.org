#+hugo_base_dir: ./
#+STARTUP: nolatexpreview
* About
:PROPERTIES:
:export_hugo_section: ./
:END:

** DONE (Yet Another) Intro
CLOSED: [2022-11-14 Mon 23:58]
:PROPERTIES:
:EXPORT_FILE_NAME: about
:END:

[[/images/about-photo.png]]

Hi, I'm Woosang, currently a CS undergraduate at a college in Upstate New York. My interest lies on creating resilient backend services (especially for Go), building foolproof measures for development procedures (e.g. CI pipelines), and tweaking Emacs configs.

On my spare time, I like to [[https://youtu.be/qyQLNk6QoJk][make music]], hike mountains and chill with my cats.

* Posts
:PROPERTIES:
:export_hugo_section: posts
:END:

** DONE First Post! :org:hugo:@scribbles:
CLOSED: [2022-11-12 Sat 18:56]
:PROPERTIES:
:EXPORT_FILE_NAME: first-post
:END:

I used to maintain a rather clunky blog, which was a mixture of =Gulp 4= and =Sass=, but finally gathered my willpower to move on to a lightweight platform.
+ Due to all the heavy assets, the blog template itself (devoid of any content) took up about 10MB. Some parts were brought from a obscure source, so I really did not have a clear understanding of what was going on under the hood.
+ Adding a new page was a pain with the absence of an universial template; dealing with all the HTML tags and what not. I wanted a platform where I could easily jot down stuff, not a polished space without any real content.
That was when I ran into [[https://gohugo.io][Hugo]] paired with [[https://ox-hugo.scripter.co/][ox-Hugo]], a backend that exports Org-mode docs in Emacs to Hugo-compatible MD files. And after my first take, I can't help but appreciate the convenience!
+ The whole [[https://github.com/paul-kang-1/pr-website][Blog repo]] stays under 1KB, and I can leverage tools like =Org-capture= to easily create new posts.
+ With the below command and some use of [[https://ox-hugo.scripter.co/doc/auto-export-on-saving/][autosave setup]], you can introduce hot loading to the local development.
  #+begin_src sh
  hugo server --buildDrafts --navigateToChanged
  #+end_src
Anyway, it feels like I've finally found a nice and cozy setup. Let's see where this leads to!

** TODO Dependency Injection in Go Microservices :dev:go:@TIL:
:PROPERTIES:
:EXPORT_FILE_NAME: dependency-injection
:END:

*** Breaking the Code Coupling
Prior to adopting the Dependency Injection pattern, "unit testing" the codebase was virtually impossible, mainly due to its external dependencies (e.g. Firebase Cloud Messaging client), which had to be initialized solely to test a small snippet of newly written code.

*** Enforcing The Repository Pattern
Although the previous codebase vaguely followed the idea of the Repository Pattern, it failed to

#+begin_src go
type (
	HallService interface {
		CreateHallWithTx(
			ctx context.Context, sqlTx boil.ContextExecutor, option repositories.HallOption
		) (*models.Hall, error)
		GetHallWithTxOf(
			ctx context.Context, sqlTx boil.ContextExecutor, userID int
		) (*models.Hall, error)
	}

	HallServiceInstance struct {
		hallRepository repositories.HallRepository
	}
)

func NewHallService(h repositories.HallRepository) *HallServiceInstance {
	return &HallServiceInstance{h}
}
#+end_src

*** Facilitating DI with Wire
Although DI improves code readability and eases up the testing procedure, it doesn't exactly scale well as the complexity of the dependency graph increases. Wire provides a number of key advantages compared to manual DI, or other tools like Uber's [[https://github.com/uber-go/dig][dig]]:
+ **Compile-time injection**: It's always to find any discrepancies in the dependency graph prior to running the actual application.
+ **Enhanced readability**: In many cases, there will be several /initialization groups/

*** References
+ [[https://learn.microsoft.com/en-us/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/infrastructure-persistence-layer-design#the-repository-pattern][The Repository Pattern]]
+ [[https://github.com/google/wire/blob/main/docs/guide.md][Google Wire Guide]]

** TODO Testing Repository Pattern Softwares in Go (1/2) :dev:go:testing:@TIL:
:PROPERTIES:
:EXPORT_FILE_NAME: repository-pattern-testing-1
:END:

It is fairly easy to perform unit tests in the context of a Repository pattern-based software: simply create mock dependencies with predefined behavior, and initialize the target service with them to check the code logic in a vacuum.

** TODO Testing Repository Pattern Softwares in Go (2/2) :dev:go:testing:@TIL:
:PROPERTIES:
:EXPORT_FILE_NAME: repository-pattern-testing-2
:END:

While the last post mostly focused on unit testing, which let us test code that we exclusively wrote in a vacuum, one other venue of testing that may be worth exploring would be **integration testing**.

** TODO CI Pipeline for Monorepo-based Microservices
:PROPERTIES:
:EXPORT_FILE_NAME: ci-pipeline-for-monorepo-based-microservices
:END:

WIP Content

** DONE Understanding Different Consistency Guarantees :distributed:@systems:
CLOSED: [2023-04-24 Mon 01:14]
:PROPERTIES:
:EXPORT_FILE_NAME: linearizability-and-varying-degrees-of-consistencies
:END:

 When it comes to implementing distributed systems, there are a whole variety of consistency models to choose from. Going through papers on system implementations of varying degrees of consistency guarantees (e.g.[[http://www.cs.cornell.edu/courses/cs5414/2017fa/papers/Spanner.pdf][Spanner]] or [[http://www.cs.utexas.edu/~lorenzo/corsi/cs380d/papers/p172-terry.pdf][Bayou]]), I found myself mixing up strictly different terms and models. To prevent further confusion, I thought it would be a good idea to cover some key terminologies here.

*** What /is/ Consistency?
There are myriads of different consistency guarantees, but what /is/ consistency in the context of distributed systems in the first place? Different definitions may exist, but I found the following the clearest: **consistency** is a test on the execution of operations[fn:1] (WLOG, let's limit the type of operations to ~read()~ and ~write(v)~ for the sake of simplicity): if the test for a consistency condition $C$ passes on execution $e$, we say $e$ is $C$-consistent.

We can also define hierarchies between different consistency semantics: $C_s$ is /stronger/ than $C_w$ if and only if the set of executions accepted by $C_s$ is a subset of the set of executions accepted by $C_w$. ($E_{C_s}\subset E_{C_w}$) If neither of them is stronger, than the two are incomparable.

*** Causal Consistency
Using Lamport's /happened-before/ relation, we can define a consistency semantic. As the [[https://www.cs.cornell.edu/lorenzo/papers/cac-tr.pdf][CAC]] paper states, an execution is /causally consistent/ if $\exists$ a DAG $G$, a happens-before graph defined by the /precedes/ partial ordering ($\succ_G$), satisfies the following check:

+ Serial ordering at each node: If $v$ and $v^{\prime}$ are vertices corresponding to operations by the same node, $v.startTime < v^{\prime}.startTime \Leftrightarrow v\prec_G v^{\prime}$.
+ Read returns the latest preceding concurrent writes. Note that this doesn't place any restrictions  on the ordering of each of the concurrent writes.

The second point essentially /separates consistency from conflict resolution/, as in the responsibility of resolving order between the concurrent writes is passed to the individual nodes. So there is **no guarantee of a total ordering** in an execution that is causally consistent; as long as the partial ordering defined by a happened-before relation is satisfied, different nodes may observe different permutations of a valid execution.

**** Real-time-causal Consistency (RTC)
We could also add a real-time requirement to the consistency test regarding the happened-before graph above. An execution $e$ is /RTC consistent/ if the HB graph satisfies this additional property:
+ $\forall u, v: u.endTime < v.startTime \Rightarrow v \nprec_G u$

*** Sequential Consistency (Lamport)
Unlike causal consistency, sequential consistency constrains the execution to be in some /total order/, and the resulting execution should be consistent with the order of operations on each individual nodes.

*** Linearizability

*** External Consistency (Gifford)

*** Serializability

*** Footnotes
[fn:1] Adopted from [[https://www.cs.cornell.edu/lorenzo/papers/cac-tr.pdf][Consistency, Availability and Convergence (Marajan et al.)]]

*** Further Readings
+ [[https://jepsen.io/consistency][Consistency Models (Jepsen)]]
+ [[https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwi2mqernbX-AhXQMlkFHSzDAQoQFnoECAwQAQ&url=https%3A%2F%2Fcs.brown.edu%2F~mph%2FHerlihyW90%2Fp463-herlihy.pdf&usg=AOvVaw2I8TvobQuAizpu3MojvSZO][Linearizability: A Correctness Condition for Concurrent Objects (Herilhy)]]
+ [[https://www.cs.cornell.edu/lorenzo/papers/cac-tr.pdf][Consistency, Availability and Convergence (Marajan et al.)]]

** DONE C++ Setup for Neovim :dev:vim:@TIL:
CLOSED: [2023-06-12 Mon 01:43]
:PROPERTIES:
:EXPORT_FILE_NAME: c-plus-plus-setup-for-neovim
:END:

There are a plethora of different ways to setup a C++ development environment in Neovim. Here's one possible way that I landed on after a number of (unsuccessful) attempts on Linux, integrated as a part of my [[https://github.com/paul-kang-1/dotfiles][dotfiles]].

*** Mason and Lsp-Zero (optional)
[[https://github.com/williamboman/mason.nvim][mason.nvim]] is a package manager for Neovim that enables the installation of different utilities (mainly LSP/DAP servers and linter/formatters).

[[https://github.com/VonHeikemen/lsp-zero.nvim][lsp-zero.nvim]] provides a sweet spot between an out-of-the-box experience and configurability for setting up language-specific functionalities. The barebones plugin configuration with support for Mason for [[https://github.com/wbthomason/packer.nvim][packer.nvim]]
is as below ([[https://github.com/VonHeikemen/lsp-zero.nvim#quickstart-for-the-impatient][source]]):

#+begin_src lua
use {
    'VonHeikemen/lsp-zero.nvim',
    branch = 'v2.x',
    requires = {
        -- LSP Support
        { 'neovim/nvim-lspconfig' }, -- Required
        {
            -- Optional
            'williamboman/mason.nvim',
            run = function()
                pcall(vim.cmd, 'MasonUpdate')
            end,
        },
        { 'williamboman/mason-lspconfig.nvim' }, -- Optional

        -- Autocompletion
        { 'hrsh7th/nvim-cmp' }, -- Required
        { 'hrsh7th/cmp-nvim-lsp' }, -- Required
        { 'hrsh7th/cmp-buffer' }, -- Optional
        { 'hrsh7th/cmp-path' }, -- Optional
        { 'saadparwaiz1/cmp_luasnip' }, -- Optional
        { 'hrsh7th/cmp-nvim-lua' }, -- Optional

        -- Snippets
        { 'L3MON4D3/LuaSnip' },    -- Required
        { 'rafamadriz/friendly-snippets' }, -- Optional
    }
}
#+end_src


*** Setting up ~clangd~ and ~clang-format~
After the prerequisites are installed, there may not be any immediate changes. That is because you'll have to provide the Clang compiler, which ~clangd~ is based on, with explicit guidance on compilation. There are different ways to supply the compilation flags to ~clangd~, but for a simple sandbox, a =.clangd= file (or a =compile_flags.txt=) may suffice. A Cmake-generated =compile_commands.json= compilation database file could also do the job for larger projects. The flags are apparently version/platform specific, so double check the system settings!

#+begin_src shell
CompileFlags:
  Add: [-std=c++20, -Wall, -I/usr/include/c++/11, -I/usr/include/x86_64-linux-gnu/c++/11]
#+end_src

Now that the errors are gone, it's time to fine tune the formatter, ~clang-format~. This can simply be done by adding a =.clang-format= file with different [[https://clang.llvm.org/docs/ClangFormatStyleOptions.html][options]] at the root directory of the project. And that's it!

*** Further Troubleshooting
One error that took me a particularly long time figuring out the root cause was the error message =bits/c++config.h file not found= that occured in headers. I tried including the directories that apparently included the lacking file, but the issue persisted. To dig deeper, I tried compiling (with verbose mode) a small test file with the following flags with ~clang++~.

#+begin_src shell
$ clang++ --std=c++2a -Wall --verbose main.cpp -o test \
    -I/usr/include/c++/11 \
    -I/usr/include/c++/x86_64-linux-gnu/11

clang version 17.0.0 (https://github.com/llvm/llvm-project.git f5a8802fa6021ab05dd126ea64f594f84c6c90d9)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /home/woosang.kang/.local/share/llvm-project/build/bin
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/11
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/12
Selected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/12
#+end_src

So it turned out to be that I've installed and attempted to include an older version (11) of ~libstdc++~ that ~clang~ wasn't using! After installing =libstdc++-12-dev= along with =g++-12-multilib= and =gcc-12-multilib=, all the features were working seamlessly without any additional configuration files.


** DONE Fly.io Distributed System Challenge with Go (Part 1) :dev:go:@systems:
CLOSED: [2023-06-19 Mon 17:11]
:PROPERTIES:
:EXPORT_FILE_NAME: checking-out-the-fly-dot-io-distributed-system-challenge-with-go
:END:

Recently, I ran into an instresting challenge on distributed systems provided by [[https://fly.io/dist-sys/][Fly.io]]. After going through a laborious semester trying to get in touch with my [[https://www.cs.cornell.edu/courses/cs5414/2023sp/][inner Ninja]] of theory and implementation, I thought that it would be a good chance to check my understanding of the field.
- Check out [[https://github.com/paul-kang-1/flyio-distributed-challenge][my repo]] for the actual implementation in Go!

*** Part 1, 2: Echo / Unique ID Generation
These parts were really about familiarizing oneself with the [[https://github.com/jepsen-io/maelstrom][Maelstrom]] testbench, which the challenge utilizes to abstract basic node-level operations (~send~, sync/async ~rpc~, etc.).
**** Globally-Unique ID Generation
There could be a different number of approaches one could take to handle this operation in a distributed setting. My implementation was fairly simple. Given that each of the nodes have their own unique ID, each node will keep its own counter. Then, a unique ID can be easily generated by concatenating the node ID with the counter value, which is incremented on each incoming client request.

#+begin_src go
func generateID(nodeId string) string {
	count++
	return nodeId + strconv.Itoa(count)
}
#+end_src

There wasn't any complicated logic; in fact, the snippet above was pretty much the gist of it.

*** Part 3: Broadcast
Things began to ramp up from this section, as now the problems required communication between internal nodes, unlike the previous problems which only required exclusive communication between the clients. (Protocol specification available [[https://fly.io/dist-sys/3a/][here]])
**** Naive Broadcast
#+name: fig__dist1
#+caption: Client sends a request to broadcast "3"
[[/images/dist1.jpeg]]

The simplest way (aka the bare minimum) to broadcast a message as a node in a distributed system would be to relay the message to its neighbors *whenever a new message is received*. The diagram above shows that case: client 1 sends a request to broadcast 3, and node 1, as the initial recipient of tha message, will relay 3 to its neighbors. One thing to be careful when forwarding the message here would be to refrain from sending back the same message to its sender, since that would cause an infinite loop - ~n1~ sends 3 to ~n2~, ~n2~ relays it to ~n3~, ~n3~ to ~n1~ and so on.

Of course, this approach is not efficient at all. Even for a simple network as in the diagram above, we can see that ~n2~ and ~n3~ are receiving duplicate messages. For more complex networks with higher throughput, this will be a major deal breaker. It's also prone to network failures and site failures, since it is built upon the assumption that the system is fault-free.

***** Implementing in Go
The idea for this stage was pretty simple, but there were some issues worth considering in the implementation process. In order to check whether a value already has been forwarded (on the receipt of a broadcast request), there should be some sort of a database that keeps all observed (and forwarded) values.

However, a naive hashset (~map[int]any~) will be problematic. For each node, there will be a main event loop running, waiting for incoming requests. Once a message arrives, a handler for that message will be called in a separate goroutine. This means that the read/write operations to the map is most likely concurrent, which is not allowed for maps in Go. So I created a wrapper for the map, along with its own mutex as below.

#+begin_src go
type MapStruct[K comparable, V any] struct {
	sync.RWMutex
	M map[K]V
}

func (m *MapStruct[K, V]) Get(key K) (value V, ok bool) {
	m.RLock()
	defer m.RUnlock()
	value, ok = m.M[key]
	return
}

// ...omitted
#+end_src

This resolved the concurrent map read/write issue, and when run on a system of 5 nodes with a load of 10 messages/sec, showed a ~message-per-op~ value of 10.14, meaning that it took approximately 10 messages internally to broadcast. However, when everything scaled up (25 nodes, 100 messages/sec), the ~message-per-op~ value grew to 75.6. Not particularly impressive, but the test run showed that the broadcast protocol is in fact functional.
**** Partition Tolerance
#+name: fig__dist2
#+caption: Broadcasting in the presence of a network partition
[[/images/dist2.jpeg]]

There could be all kinds of nemesis a distributed system may encounter, and /network partition/ is definitely one of them. A *network partition*[fn:1] happens when the operational nodes are divided into networks of two or more fully-connected components (cliques), and communication between those components are disabled. The previously designed system will be unable to survive this failure, as it only relays messages to its neighbors once. If the neighbor was partitioned during the arrival of a new message, it won't ever be sent again, breaking consistency between different nodes across the partition.

How could we build a system that is [[https://en.wikipedia.org/wiki/Eventual_consistency][eventually consistent]] in the presence of such failures? There may be different ways to address this issue, but retries could be a solution. Instead of mindlessly throwing values at one's neighbors upon the receipt of a new message, it could expect an acknowledgement from each neighbor to make sure that it received the value. That way, the nodes will keep trying to reach the neighbors on the other side of the partition, and ultimately to sync up on the values when the partition heals.

***** Implementation in Go
For each incoming message, the ~broadcast~ message handler now made use of the ~RPC()~ call, which comes with its own handler to process the ACK message returned by its neighbor. When a broadcast request for a new value arrives, a node will do the following:
- Create a map of neighbors (~waiting~) to relay the broadcast and expect an ACK from
- Retry the relay RPC (only for those that haven't returned an ACK yet) until all recipients sends back an ACK
Although the logic wasn't that complicated, it was tricky to evade the concurrent read/write issue for the ~waiting~ map, especially since it had to iterate over each key/values. I could've dealed with this issue by placing mutex locks, but decided to check out the ~sync.Map~ provided by Go, which seems to be recommended for a limited use case (stated below) over the traditional Go Map paired with mutexes:
- When the entry for a given key is only ever written once but read many times, as in caches that only grow
- When multiple goroutines read, write, and overwrite entries for disjoint sets of keys.
Since my use case perfectly fit the first one, I decided to track the ACK message receipt progress with it, as in the code snippet below:

#+begin_src go
// Excerpt from the broadcast handler logic
db.Put(message, nil)
waiting := sync.Map{} // map[string]bool (neighbor addr: is ACK arrived)
for _, neighbor := range neighbors {
    if neighbor == msg.Src {
        continue // Exclude sender from message relay recipient
    }
    waiting.Store(neighbor, false)
}
pending := true
var err error
for pending {
    pending = false
    waiting.Range(func(neighbor, value any) bool {
        if v, _ := waiting.Load(neighbor); v.(bool) {
            return true
        }
        pending = true
		// Does not block (asynchronous call)
        err = n.RPC(neighbor.(string), body, func(msg maelstrom.Message) error {
            waiting.Store(neighbor, true)
            return nil
        })
        return err == nil
    })
    time.Sleep(time.Millisecond * 500)
}
if err != nil { return err }

// ...omitted
#+end_src

After making the change above, the system was able to synchronize itself after the healing of a network partition. It still wasn't an efficient system overall (55 ~message-per-op~ , median/maximum latency of 460, 793ms), but that wasn't the ultimate objective for this part.

On my next post, I'll go through how I made the system to become more competent!

*** Footnotes
[fn:1] Adopted from P.Bernstein, N.Goodman and V.HAdzilakos, /Distributed Recovery/

** DONE Fly.io Distributed System Challenge with Go (Part 2) :dev:go:@systems:
CLOSED: [2023-06-29 Thu 23:59]
:PROPERTIES:
:EXPORT_FILE_NAME: fly-dot-io-distributed-system-challenge-with-go-part-2
:END:

In my previous post, I covered how I built a basic partition-tolerant broadcast system. While it did manage to perform correctly, it was not exactly performant. There was plenty of room for performance optimizations that could be done - this post covers them.

*** Efficiency Metrics
Maelstrom, the underlying testbench for the challenge, provided a lot of metrics and charts that could be used to analyze the performance of my algorithm. Here are some of the key metrics:
- *Stable latency* is a measure of time elapsed for a message to be propagated to all nodes (i.e., visible in the output of ~read~ operation on every nodes). The latency is displayed in percentiles. For example, a ~stable-latencies~ field with ~{0 0, 0.5 100, 0.95 200, 0.99 300, 1 400}~ would indicate a median latency of 100ms, and a maximum of 400ms.
- *Message per operation* is the outcome of dividing the total number of messages exchanged between servers with the number of requests (note that the request count also includes ones that does /not/ require any inter-server communication, such as ~read~). So if we have the same number of reads and non-read operations, we have to /double/ the number to get the actual ~message-per-op~ for broadcasts.

With these criteria under the belt, it was possible to assess the performance of the implementation with better accuracy. The first objective of the performance optimization was to have a ~message-per-op~ under 30, and median & maximum latency of 400ms and 600ms, respectively.

*** Optimization #1: Redefining the Network Topology
Closely reviewing the problem description, I saw that I could ignore the =topology= message and define my own network! This was significant in many ways.
**** The more connections, the better?

#+name: fig__dist3
#+caption: More connections may lead to unnecessary message exchanges
[[/images/dist3.jpeg]]

Although the idea of having a fully connected network sounds enthralling, utilizing it in itself may not be the most efficient choice. The biggest culprit, as visible in the left network in the figure above, is the existence of /loops/. Loops lead to unnecessary sends/receipts of messages, increasing the ~message-per-op~ count. A less-connected network in the right, in fact, shows better efficiency in broadcasting a single message. If that's the case, how could one create a loop-free network?

**** Trees to the rescue
Well, some might have seen it coming, but [[https://en.wikipedia.org/wiki/Spanning_tree][spanning trees]] could do the job here. The loopless property of trees fit perfectly to the situation, and the fact that it spans all nodes makes it a functional network. In fact, it already is being used widely in communication networks, namely the [[https://en.wikipedia.org/wiki/Spanning_Tree_Protocol][Spanning Tree Protocol (STP)]].

In the context of this problem, we could simply ignore the =topology= message and build a spanning tree. Since each node has information about all nodes that constitute the system, it can simply build a tree (and decide which neighbors to be a parent/children) by itself, unlike STP.

#+name: fig__dist4
#+caption: Spanning tree construction with 5 nodes, max 2 children/node
[[/images/dist4.jpeg]]

I parameterized the number of children each node can have, and tried tuning these values (=num_children=). If you crank up this value, the resulting tree will be shallow in depth, which could help the message to propagate faster throughout the network - to an extent.

Contrary to my belief, when I set =num_children= to be =n-1=, i.e., the network will be depth 1 with node 0 being the root, and all others connected to it, the median and maximum stable latency actually increased. This may be due to the increased load given to node 0, which would have to handle basically everything by itself. Even when each of the handlers were handled in its own goroutine, it yielded degraded performance.

The optimal =num_children= for 25 nodes turned out to be between 3 and 5, which would lead to 2-3 level-deep spanning trees, which led to a server =msgs-per-op= of 22.85, and median and maximum latency of 398 and 403ms. It barely passed the median latency requirement (400ms), but not bad otherwise!

#+begin_src edn
:net {:all {:send-count 48228,
            :recv-count 48228,
            :msg-count 48228,
            :msgs-per-op 24.911158},
    :clients {:send-count 3972, :recv-count 3972, :msg-count 3972},
    :servers {:send-count 44256,
                :recv-count 44256,
                :msg-count 44256,
                :msgs-per-op 22.859505},
    :valid? true},
:stable-latencies {0 73, 0.5 357, 0.95 398, 0.99 401, 1 403},
#+end_src

*** Optimization #2: Rethinking Inter-Node Communication
For the last section, the bar for efficiency got even higher, with ~message-per-op~ less than 20. However, there was a trade-off in *latency*, as the bar for the median and maximum latency was now one and two seconds, respectively.

**** Rethinking inter-node communication
Until now, there had to be a message exchange (~broadcast~ request) whenever a node saw a new incoming message. That may help in propagating a message ASAP, resulting in a better stable latency distribution, but it doesn't help a lot when it comes to efficiency in terms of message counts. How could we save to the extreme, sacrificing some of the latency if needed?

The first idea that came into my mind was /message batching/. Instead of sending ~broadcast~ request on every new message, we could collect new messages until its size equals a predefined ~BATCH_SIZE~ constant. /Then/ we could send out the /set/ of new messages collected to the neighbors.

However, relying solely on the batch size as a criterion for sending out messages can be dangerous. If clients send messages just short of ~BATCH_SIZE~ and stop sending, there is no way for the node to propagate the messages that it's holding - breaking the critical [[https://en.wikipedia.org/wiki/Safety_and_liveness_properties][liveness]] requirement.

**** Psst! Psst!
The main problem from the previous approach was the lack of a /temporal/ demension. Instead of having a upper bound on message counts, we can have a bound on the /exchange period/. In other words, the nodes will sync with each other periodically, with the set of messages they have at the moment of synchronization.

Alright, will that save us a bunch of messages? Well...not yet. This method of naive sharing will lead to an non-decreasing message size, which will quickly grow impractical as the messages aggregate through time. Instead, the nodes act as if they share /gossip/. You don't gossip with someone that already knows the story - you only share with those who haven't (or at least you think they haven't) heard of the news.

So /periodic gossip/, a family of the [[https://en.wikipedia.org/wiki/Gossip_protocol][gossip protocol]], will be an effective strategy here. In order to make this happen, the nodes would need a separate database of *who knows what* for each of its neighbor (~acked~ in the snippet below). And then, periodically, each node would gossip to its neighbors a customized set of messages that is presumed to be new to them.

#+name: fig__dist5
#+caption: Gossip protocol between three nodes (persp. of =n1=)
[[/images/dist5.jpeg]]

The neighbor that receives the message can then send an =ACK= of the messages back to the node, which will update its 'who knows what' database. Here's an implementation of what I just said in Go:

#+begin_src go
// 'Set' of values that this node knows (the `any` is a placeholder)
db utils.MapStruct[int, any]
// keep a record of who knows what (for neighbors)
acked utils.MapStruct[string, map[int]any]

func syncDB(n *maelstrom.Node) error {
	// ...
	values := *db.Keys() // all values that I know at the moment
	body := make(msgBody)
	var message []int
	var currAcked map[int]any // set of values a neighbor knows
	// customize message sending per each neighbor
	for _, neighbor := range neighbors {
		message = make([]int, 0)
		currAcked = make(map[int]any)
		// The generic structure of the MapStruct type make it
		// impossible to support iteration on a single map value
		// without exposing the embedded mutex and map
		acked.RLock()
		for val := range acked.M[neighbor] {
			currAcked[val] = nil //
		}
		acked.RUnlock()
		for _, v := range values {
			if _, ok := currAcked[v]; !ok {
				message = append(message, v)
			}
		}
		body["message"] = message
		if err := n.Send(neighbor, &body); err != nil {
			return err
		}
	}
	return nil
}
#+end_src

With ~syncDB()~ defined, we could make the node to synchronize messages with its neighbors periodically by adding another event loop as below:

#+begin_src go
// excerpt from main()
go func() {
    for {
        if err := syncDB(n); err != nil {
            log.Fatal(err)
        }
        time.Sleep(SyncMs * time.Millisecond)
    }
}()
#+end_src

**** Final results
The end result turned out to be much better than I expected: a whopping ~message-per-op~ value of *2.98* (compare that to the previous 22.85, which was already optimized from the older version!), and median/maximum stable latency of 1001ms and 1129ms, respectively.

#+begin_src edn
:net {:all {:send-count 9718,
            :recv-count 9715,
            :msg-count 9718,
            :msgs-per-op 5.037843},
    :clients {:send-count 3958, :recv-count 3958, :msg-count 3958},
    :servers {:send-count 5760,
                :recv-count 5757,
                :msg-count 5760,
                :msgs-per-op 2.9860032},
    :valid? true},

:stable-latencies {0 0, 0.5 895, 0.95 1001, 0.99 1099, 1 1129},
#+end_src

So it passes the final hurdle of < 20 =messages-per-op=, and the median/maximum stable latency requirement with flying colors. Yay!

*** Next Up: Grow-Only Counter
That concludes the long journey to implementing a performant, partition-tolerant broadcast system. On my next post, I'll share how I struggled with the subtleties of sequential consistency, and eventually built a distributed, [[https://fly.io/dist-sys/4/][Grow-Only Counter]].

** TODO Fly.io Distributed System Challenege with Go (Part 3) :dev:go:@systems:
:PROPERTIES:
:EXPORT_FILE_NAME: fly-dot-io-distributed-system-challenege-with-go-part-3
:END:

 Time flies for sure -
*** Topics to Cover
+ Completing challenge part 4
+ Sequential Consistency
  - Zookeeper: $OSC(U)$: Ordered sequential consistency on updates
  - Maelstrom's ~seq-kv~ service traits

** TODO Different Types of Build Systems :dev:bazel:@devops:
:PROPERTIES:
:EXPORT_FILE_NAME: different-types-of-build-systems
:END:

Recently, I was working on a project of implementing a build system in Bazel for a large NodeJS application.
*** Task-Based Build Systems
*** Artifact-Based Build Systems
*** On Sustainability
