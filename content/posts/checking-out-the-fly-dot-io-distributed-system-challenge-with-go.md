+++
title = "Fly.io Distributed System Challenge with Go (Part 1)"
author = ["Woosang Kang"]
tags = ["dev", "go"]
categories = ["systems"]
draft = true
+++

Recently, I ran into an instresting challenge on distributed systems provided by [Fly.io](https://fly.io/dist-sys/). After going through a laborious semester trying to get in touch with my [inner Ninja](https://www.cs.cornell.edu/courses/cs5414/2023sp/) of theory and implementation, I thought that it would be a good chance to check my understanding of the field.


## Part 1, 2: Echo / Unique ID Generation {#part-1-2-echo-unique-id-generation}

These parts were really about familiarizing oneself with the [Maelstrom](https://github.com/jepsen-io/maelstrom) testbench, which the challenge utilizes to abstract basic node-level operations (`send`, sync/async `rpc`, etc.).


### Globally-Unique ID Generation {#globally-unique-id-generation}

There could be a different number of approaches one could take to handle this operation in a distributed setting. My implementation was fairly simple. Given that each of the nodes have their own unique ID, each node will keep its own counter. Then, a unique ID can be easily generated by concatenating the node ID with the counter value, which is incremented on each incoming client request.

```go
func generateID(nodeId string) string {
	count++
	return nodeId + strconv.Itoa(count)
}
```

There wasn't any complicated logic; in fact, the snippet above was pretty much the gist of it.


## Part 3: Broadcast {#part-3-broadcast}

Things began to ramp up from this section, as now the problems required communication between internal nodes, unlike the previous problems which only required exclusive communication between the clients. (Protocol specification available [here](https://fly.io/dist-sys/3a/))


### Naive Broadcast {#naive-broadcast}

<a id="figure--dist1"></a>

{{< figure src="/images/dist1.jpeg" caption="<span class=\"figure-number\">Figure 1: </span>Client sends a request to broadcast \"3\"" >}}

The simplest way (aka the bare minimum) to broadcast a message as a node in a distributed system would be to relay the message to its neighbors **whenever a new message is received**. The diagram above shows that case: client 1 sends a request to broadcast 3, and node 1, as the initial recipient of tha message, will relay 3 to its neighbors. One thing to be careful when forwarding the message here would be to refrain from sending back the same message to its sender, since that would cause an infinite loop - `n1` sends 3 to `n2`, `n2` relays it to `n3`, `n3` to `n1` and so on.

Of course, this approach is not efficient at all. Even for a simple network as in the diagram above, we can see that `n2` and `n3` are receiving duplicate messages. For more complex networks with higher throughput, this will be a major deal breaker. It's also prone to network failures and site failures, since it is built upon the assumption that the system is fault-free.


#### Implementing in Go {#implementing-in-go}

The idea for this stage was pretty simple, but there were some issues worth considering in the implementation process.


### Partition Tolerance {#partition-tolerance}

There could be all kinds of nemesis a distributed system may encounter, and _network partition_ is definitely one of them. A network partition[^fn:1] happens when the operational nodes are divided into networks of two or more fully-connected components (cliques), and communication between those components are disabled.


### Efficiency Metrics {#efficiency-metrics}

Maelstrom, the underlying testbench for the challenge, provided a lot of metrics and charts that could be used to analyze the performance of my algorithm. Here are some of the key metrics:

-   **Stable latency** is a measure of time elapsed for a message to be propagated to all nodes (i.e., visible in the output of `read` operation on every nodes). The latency is displayed in percentiles. For example, a `stable-latencies` field with `{0 0, 0.5 100, 0.95 200, 0.99 300, 1 400}` would indicate a median latency of 100ms, and a maximum of 400ms.
-


### Optimization #1: Reshaping the Network {#optimization-1-reshaping-the-network}

Closely reviewing the problem description, I saw that I could ignore the `topology` message and define my own network.


### Optimization #2: Periodic Gossip {#optimization-2-periodic-gossip}

For the last section, the bar for efficiency got even higher, with `message-per-op` less than 20. However, there was a trade-off in latency, as the bar for the median and maximum latency was now one and two seconds, respectively.

```edn
:net {:all {:send-count 9718,
            :recv-count 9715,
            :msg-count 9718,
            :msgs-per-op 5.037843},
    :clients {:send-count 3958, :recv-count 3958, :msg-count 3958},
    :servers {:send-count 5760,
                :recv-count 5757,
                :msg-count 5760,
                :msgs-per-op 2.9860032},
    :valid? true},

:stable-latencies {0 0, 0.5 895, 0.95 1001, 0.99 1099, 1 1129},
```

[^fn:1]: Adopted from P.Bernstein, N.Goodman and V.HAdzilakos, _Distributed Recovery_